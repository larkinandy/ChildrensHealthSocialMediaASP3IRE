{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2033d140",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as ps\n",
    "import os\n",
    "import numpy as np \n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "import tensorflow as tf\n",
    "import transformers\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee2d44",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mapTextExtract(tweet):\n",
    "    tmpText = tweet['text'].replace(\"\\n\",\". \")\n",
    "    \n",
    "    return(tmpText + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40175bcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mapConvExtract(tweet):\n",
    "    try:\n",
    "        convid = tweet['conversation_id']\n",
    "    except:\n",
    "        convid = 'nope'\n",
    "    return(convid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848b451",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def mapIdExtract(tweet):\n",
    "    return(tweet['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0276c31",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processSingleYearSingleVariable(tuple):\n",
    "    year = tuple[0]\n",
    "    variable = tuple[1]\n",
    "    folder = \"Z:/Aspire/TweetStore/\" + str(year) + \"/\" + str(variable) + \"/\"\n",
    "    outputFile = \"H:/Aspire/BERT/BERT_text/txt_\" + str(year) + \"_\" + str(variable) + \".txt\"\n",
    "    if(os.path.exists(outputFile)):\n",
    "        print(\"%s already exists\" %(outputFile))\n",
    "        return\n",
    "    tweetFiles = glob.glob(folder + \"tw_*\")\n",
    "    text = []\n",
    "    print(\"found %i files for variable %s and year %i\" %(len(tweetFiles),variable,year))\n",
    "    for file in tweetFiles:\n",
    "        text += combineSingleDay(file)\n",
    "    writeToText(text,outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8666a95c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def loadSingleBatch(filename):\n",
    "    tweetBatch = np.load(filename,allow_pickle=True)\n",
    "    text,convId,tweetId = [],[],[]\n",
    "    for subBatch in tweetBatch:\n",
    "        text +=  list(map(mapTextExtract,subBatch))\n",
    "        convId += list(map(mapConvExtract,subBatch))\n",
    "        tweetId += list(map(mapIdExtract,subBatch))\n",
    "    df = ps.DataFrame({\n",
    "        'text':text,\n",
    "        'convId':convId,\n",
    "        'tweetId':tweetId\n",
    "    })\n",
    "    if(df.count()[0]>0):\n",
    "        df = df[~df['convId'].str.contains('nope')]\n",
    "    return(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ee1b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def setupModelInputs(tweetData,debug=False):\n",
    "    tweetText = list(tweetData['text'])\n",
    " #   tweetText = list(map(mapNewLineReplace,tweetText))\n",
    "    inputs = TOKEN(tweetText,max_length=100,truncation=True,padding='max_length',return_tensors=\"tf\")\n",
    "    inp_ids = tf.convert_to_tensor(inputs['input_ids'])\n",
    "    inputs['input_ids'] = inp_ids\n",
    "    return(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d84ac8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def combineSingleDay(inFile):\n",
    "    text = []\n",
    "    npData = np.load(inFile,allow_pickle=True)\n",
    "    for batch in npData:\n",
    "        text += list(map(mapTextExtract,batch))\n",
    "    print(\"found %i tweets for day %s\" %(len(text),inFile))\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd26503",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processSingleYearSingleVariable(year,variable,model):\n",
    "    inFolder = \"/mnt/z/Aspire/TweetStore/\" + str(year) + \"/\" + str(variable) + \"/\"\n",
    "    outFolder = \"/mnt/h/Aspire/PredStore/\" + str(year) + \"/\" + str(variable) + \"/\"\n",
    "    tweetFiles = glob.glob(inFolder + \"tw_*\")\n",
    "    for file in tweetFiles:\n",
    "        dateStamp = file[file.rfind('_')+1:-4]\n",
    "        outputFile = outFolder + \"re_\" + dateStamp + \".csv\"\n",
    "        if(os.path.exists(outputFile)):\n",
    "            print(\"%s already exists\" %(outputFile))\n",
    "        else:\n",
    "            processSingleBatch(file,outputFile,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c63e32",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "modelFile = '/mnt/h/Aspire/BERT/health/health_model_checkpoint.h5'\n",
    "relaxedBERT = tf.keras.models.load_model(modelFile,custom_objects={\"TFBertModel\": transformers.TFBertModel})\n",
    "relaxedBERT.compile(loss=[tf.keras.losses.BinaryCrossentropy()],optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),metrics=['accuracy'])\n",
    "TOKEN = BertTokenizer.from_pretrained('/mnt/h/Aspire/BERT/health/expandedTokenBase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dad8480",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processSingleConvFolder(folder,model):\n",
    "    inFolder = \"/mnt/z/Aspire/TweetStore/conversations/\" + folder + \"/\"\n",
    "    outFolder = \"/mnt/h/Aspire/PredStore/conversations/\" + folder + \"/\"\n",
    "    tweetFiles = glob.glob(inFolder + \"tw_*\")\n",
    "    for file in tweetFiles:\n",
    "        convId = file[file.rfind('_')+1:-4]\n",
    "        outputFile = outFolder + \"re_\" + convId + \".csv\"\n",
    "        if(os.path.exists(outputFile)): \n",
    "            a=1\n",
    "         #   print(\"%s already exists\" %(outputFile))\n",
    "        else:\n",
    "            processSingleBatch(file,outputFile,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2104cef",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def processSingleBatch(inFile,outFile,model):  \n",
    "    df = loadSingleBatch(inFile)\n",
    "    if(df.count()[0]==0):\n",
    "        #print(\"%s has 0 records\" %(inFile))\n",
    "        return\n",
    "    modelInputs = setupModelInputs(df)\n",
    "    preds = model.predict([modelInputs['input_ids'],modelInputs['attention_mask']])\n",
    "    df['isCognitive'] = tf.nn.sigmoid(preds[:,0])\n",
    "    df['isEmotional'] = tf.nn.sigmoid(preds[:,1])\n",
    "    df['isPhysical'] = tf.nn.sigmoid(preds[:,2])\n",
    "    df['isPositive'] = tf.nn.sigmoid(preds[:,3])\n",
    "    df['isNegative'] = tf.nn.sigmoid(preds[:,4])\n",
    "    df2 = df.drop(['text'],axis=1)\n",
    "    df2.to_csv(outFile,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13552a0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = np.load(\"/mnt/z/Aspire/TweetStore/2016/health/tw_20160204.npy\",allow_pickle=True)\n",
    "index=0\n",
    "for b in a:\n",
    "    for c in b:\n",
    "        try:\n",
    "            d = c['conversation_id']\n",
    "        except:\n",
    "            print(index)\n",
    "        index+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1ba02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parentConvFolder = '/mnt/z/Aspire/TweetStore/conversations/'\n",
    "outputFolder = '/mnt/h/Aspire/PredStore/conversations/'\n",
    "conversationFolders = os.listdir(parentConvFolder)\n",
    "index = 0\n",
    "for folder in conversationFolders:\n",
    "    curFolder = parentConvFolder + folder + \"/\"\n",
    "    if not(os.path.exists(outputFolder + folder)):\n",
    "        os.mkdir(outputFolder + folder)\n",
    "    processSingleConvFolder(folder,relaxedBERT)\n",
    "    index+=1\n",
    "    if(index%100==0):\n",
    "        print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab72d1e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   2/1137 [..............................] - ETA: 1:21 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-31 15:50:15.842729: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1137/1137 [==============================] - 42s 36ms/step\n",
      "1204/1204 [==============================] - 43s 36ms/step\n",
      "1333/1333 [==============================] - 49s 36ms/step\n",
      "1379/1379 [==============================] - 49s 35ms/step\n",
      "1387/1387 [==============================] - 49s 35ms/step\n",
      "1386/1386 [==============================] - 51s 36ms/step\n",
      "1356/1356 [==============================] - 50s 37ms/step\n",
      "1132/1152 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "for year in reversed(range(2014,2019)):\n",
    "    yearFolder = '/mnt/h/Aspire/PredStore/' + str(year) + \"/\"\n",
    "    if not(os.path.exists(yearFolder)):\n",
    "        os.mkdir(yearFolder)\n",
    "    for cat in['age','place','env','health','health2']:\n",
    "        print(\"calculating values for year %i and category %s\" %(year,cat))\n",
    "        catFolder = yearFolder + \"/\" + str(cat)\n",
    "        if not(os.path.exists(catFolder)):\n",
    "            os.mkdir(catFolder)\n",
    "        processSingleYearSingleVariable(year,cat,relaxedBERT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
