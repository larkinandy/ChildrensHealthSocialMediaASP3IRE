{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a6e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 11:57:55.099105: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-16 11:57:55.277867: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-16 11:57:55.912173: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64:/usr/lib/wsl/lib::/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-10-16 11:57:55.914214: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64:/usr/lib/wsl/lib::/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-10-16 11:57:55.914220: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "# Setting random seeds to reduce the amount of randomness in the neural net weights and results\n",
    "# The results may still not be exactly reproducible\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fcf526f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 11:57:56.816619: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:56.934557: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:56.934600: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.591290: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.591787: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.591797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-16 11:57:57.591829: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.591858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 21290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Testing to ensure GPU is being utilized\n",
    "# Ensure that the Runtime Type for this notebook is set to GPU\n",
    "# If a GPU device is not found, change the runtime type under:\n",
    "# Runtime>> Change runtime type>> Hardware accelerator>> GPU\n",
    "# and run the notebook from the beginning again.\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f45b9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def convertCodeToNumpy(code):\n",
    "    encoding = []\n",
    "    for charIndex in range(len(code)):\n",
    "        curCar = code[charIndex]\n",
    "        if curCar =='0':\n",
    "            encoding.append(0)\n",
    "        elif curCar =='1':\n",
    "            encoding.append(1)\n",
    "    return(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "966020ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename     encoding   \n",
      "0  /mnt/h/Aspire/Bert/health/train/images/aug/3_4...  [0 0 0 0 0]  \\\n",
      "1  /mnt/h/Aspire/Bert/health/train/images/aug/3_4...  [0 0 0 0 0]   \n",
      "2  /mnt/h/Aspire/Bert/health/train/images/aug/3_4...  [0 0 0 0 0]   \n",
      "3  /mnt/h/Aspire/Bert/health/train/images/aug/3_4...  [0 0 0 0 0]   \n",
      "4  /mnt/h/Aspire/Bert/health/train/images/aug/3_4...  [0 0 0 0 0]   \n",
      "\n",
      "            target  \n",
      "0  [0, 0, 0, 0, 0]  \n",
      "1  [0, 0, 0, 0, 0]  \n",
      "2  [0, 0, 0, 0, 0]  \n",
      "3  [0, 0, 0, 0, 0]  \n",
      "4  [0, 0, 0, 0, 0]  \n"
     ]
    }
   ],
   "source": [
    "# Importing the augmented training dataset and testing dataset to create tensors of images using the filename paths.\n",
    "train_aug_df = pd.read_csv(\"/mnt/h/Aspire/BERT/health/train/trainData.csv\")\n",
    "test_df = pd.read_csv(\"/mnt/h/Aspire/BERT/health/test/testData.csv\")\n",
    "train_aug_df['filename'] = train_aug_df['filename'].str.replace(\"H:/\", \"/mnt/h/\", case = False)\n",
    "train_aug_df['target'] = train_aug_df['encoding'].map(convertCodeToNumpy)\n",
    "test_df['target'] = test_df['encoding'].map(convertCodeToNumpy)\n",
    "test_df['filename'] = test_df['filename'].str.replace(\"H:/\",\"/mnt/h/\",case=False)\n",
    "test_labels_list = list(test_df['target'])\n",
    "print(train_aug_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "619329d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to read the image, decode the image from given tensor and one-hot encode the image label class.\n",
    "# Changing the channels para in tf.io.decode_jpeg from 3 to 1 changes the output images from RGB coloured to grayscale.\n",
    "num_classes = 5\n",
    "def _parse_function(filename, label):   \n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.io.decode_jpeg(image_string, channels=3)    # channels=1 to convert to grayscale, channels=3 to convert to RGB.\n",
    "    #image_decoded = tf.image.resize(image_decoded, [224, 224])\n",
    "    image_decoded = tf.cast(image_decoded, dtype=tf.float32)\n",
    "    \n",
    "    return(image_decoded,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf2e5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 11:57:57.937200: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937272: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937299: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937732: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937763: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937943: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937951: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-10-16 11:57:57.937979: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-10-16 11:57:57.937988: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Converting the filenames and target class labels into lists for augmented train and test datasets.\n",
    "train_aug_filenames_list = list(train_aug_df['filename'])\n",
    "train_aug_labels_list = list(train_aug_df['target'])\n",
    "test_filenames_list = list(test_df['filename'])\n",
    "test_labels_list = list(test_df['target'])\n",
    "# Creating tensorflow constants of filenames and labels for augmented train and test datasets from the lists defined above.\n",
    "train_aug_filenames_tensor = tf.constant(train_aug_filenames_list)\n",
    "train_aug_labels_tensor = tf.constant(train_aug_labels_list)\n",
    "test_filenames_tensor = tf.constant(test_filenames_list)\n",
    "test_labels_tensor = tf.constant(test_labels_list)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_filenames_tensor, test_labels_tensor))\n",
    "test_dataset = test_dataset.map(_parse_function)\n",
    "# test_dataset = test_dataset.repeat(3)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)    # Same as batch_size hyperparameter in model.fit() below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3371047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2023-10-16 11:58:01.297800: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n",
      "2023-10-16 11:58:02.110686: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\", trainable=True),\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b16_fe/1\", trainable=True),\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b32_fe/1\", trainable=False),\n",
    "     #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_r26_s32_lightaug_fe/1\", trainable=True),\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_r50_l32_fe/1\", trainable=True),\n",
    "hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\",\n",
    "               trainable=False, arguments=dict(batch_norm_momentum=0.997)),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(5,activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True)],\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),metrics=['accuracy'])#,run_eagerly=True)\n",
    "weightFile = \"/mnt/h/Aspire/BERT/health/health_image_model_checkpoint.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=weightFile,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True,\n",
    "                                verbose=1,\n",
    "                                initial_value_threshold=  None#0.97828\n",
    "                                )\n",
    "model(np.zeros((1,224,224,3)))\n",
    "if(os.path.exists(weightFile)):\n",
    "    model.load_weights(weightFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35dc89b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 2s 92ms/step\n",
      "[0. 0. 1. 1. 0.]\n",
      "[2.4412359e-01 1.0869887e-13 1.0000000e+00 1.0000000e+00 9.9998796e-01]\n"
     ]
    }
   ],
   "source": [
    "# Generating predictions from the model above.\n",
    "placeImagePred = model.predict(test_dataset)\n",
    "#final_cnn_pred = final_cnn.predict(train_aug_dataset)\n",
    "print(placeImagePred[0])\n",
    "imgBinary = (placeImagePred>0.5)*1\n",
    "print(np.max(placeImagePred,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dab8f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgBinary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f4aaa6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m             predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((predictions,pred))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m([labelSet,predictions])\n\u001b[0;32m---> 13\u001b[0m labels,predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgetPredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mplaceImagePred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m, in \u001b[0;36mgetPredictions\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m labelSet,predictions \u001b[38;5;241m=\u001b[39m [],[]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, labels \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(text)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(labelSet)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m         labelSet \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "#BERT_pred = final_BERT.predict(testData)\n",
    "def getPredictions(dataset,model):\n",
    "    labelSet,predictions = [],[]\n",
    "    for text, labels in dataset.take(1000):\n",
    "        pred = model.predict(text)\n",
    "        if(len(labelSet)==0):\n",
    "            labelSet = labels.numpy()\n",
    "            predictions = pred\n",
    "        else:\n",
    "            labelSet = np.concatenate((labelSet,labels))\n",
    "            predictions = np.concatenate((predictions,pred))\n",
    "    return([labelSet,predictions])\n",
    "labels,predictions = getPredictions(test_dataset,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b01c2385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 28 201 135 204 129]\n",
      "[  0   0 779 645   1]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(test_labels_list,axis=0))\n",
    "print(np.sum(imgBinary,axis=0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbd722c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0., 134., 171.,   0.]),\n",
       " array([ 0.        ,  0.        , 99.25925926, 83.82352941,  0.        ]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcTruePositive(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    correct = np.multiply(labels,predictionsRound)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTruePositive(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95eb875e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([753, 580,   1, 103, 651]),\n",
       " array([100.        , 100.        ,   0.15479876,  17.85095321,\n",
       "         99.84662577]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_list = np.asarray(test_labels_list)\n",
    "def calcTrueNegative(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    labZeros = np.where(labels<1,1,0)\n",
    "    predZeros = np.where(predictionsRound==0,1,0)\n",
    "    correct = np.multiply(labZeros,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labZeros,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTrueNegative(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0662a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 28, 201,   1,  33, 129]),\n",
       " array([100.        , 100.        ,   0.74074074,  16.17647059,\n",
       "        100.        ]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalseNegative(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    predZeros = np.where(predictionsRound==0,1,0)\n",
    "    correct = np.multiply(labels,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalseNegative(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7f413d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0. 645. 474.   1.]\n",
      "[ 28 201 135 204 129]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0., 645., 474.,   1.]),\n",
       " array([  0.        ,   0.        , 477.77777778, 232.35294118,\n",
       "          0.7751938 ]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalsePositive(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    labZeros = np.where(labels==0,1,0)\n",
    "    correct = np.multiply(labZeros,predictionsRound)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    print(nCorrect)\n",
    "    print(np.sum(labels,axis=0))\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalsePositive(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a79f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_41/3656412888.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testData['isImage'] = isImage\n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "import os\n",
    "allData = ps.read_csv(\"/mnt/h/Aspire/BERT/health/healthLabelsTrainTest_Oct3_23.csv\")\n",
    "testData = allData[allData['test']==1]\n",
    "isImage = []\n",
    "for curNum in range(testData.count()[0]):\n",
    "    curRecord = testData.iloc[curNum]\n",
    "    curImage = curRecord['img_http']\n",
    "    if(os.path.exists(\"/mnt/h/Aspire/BERT/health/test/images/base/\" + curImage[:-4] + \"_padded.jpg\")):\n",
    "        isImage.append(1)\n",
    "    else:\n",
    "        isImage.append(0)\n",
    "testData['isImage'] = isImage\n",
    "imageTest = testData[testData['isImage']==1]\n",
    "imageTest.to_csv('/mnt/h/Aspire/BERT/health/test/imageTweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc5ea57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4       @DBL_L_01 here is the version that @SwainEvent...\n",
      "6       Tonight I parted ways with my good friend, my ...\n",
      "11        @Ash_Tree_Lane cough hey http://t.co/8KRUqUB9YQ\n",
      "18      3+ hours &amp; counting on runway,planning nut...\n",
      "28      my face when I met @chescaleigh ðŸ˜ thank you an...\n",
      "                              ...                        \n",
      "4969    The 2019 Top Dog Tournament to benefit the Leu...\n",
      "4974    @EmerickGiveBack Portland's Emergency Coordina...\n",
      "4976    Learning a lot @SACocktailConf http://t.co/zCQ...\n",
      "4977    The imagination of children. #pirates #bubbles...\n",
      "4989    Just found out me and Tyus went to preschool t...\n",
      "Name: text, Length: 781, dtype: object\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 1]]\n",
      "[[0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 1 0 1]\n",
      " [0 1 1 0 1]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "a = imageTest['text']\n",
    "print(a)\n",
    "tweetText = list(imageTest['text'])\n",
    "tweetText = list(map(mapNewLineReplace,tweetText))\n",
    "tweetLabels = list(map(convertPlaceCodeToLabels,list(imageTest['health_type'])))\n",
    "tweetLabels2 = list(map(convertOutdoorCodeToLabels,list(imageTest['health_impact'])))\n",
    "tweetLabels3 = np.concatenate((tweetLabels, tweetLabels2),axis=1)\n",
    "print(tweetLabels3[0:10])\n",
    "print(tweetLabels3[0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0aa14f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Loading custom CUDA kernels...\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Loading custom CUDA kernels...\n",
      "Using /home/larkinan/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/larkinan/.cache/torch_extensions/py39_cu117/cuda_kernel/build.ninja...\n",
      "Building extension module cuda_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cuda_kernel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import glob\n",
    "\n",
    "# Setting random seeds to reduce the amount of randomness in the neural net weights and results\n",
    "# The results may still not be exactly reproducible\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from transformers import *\n",
    "import transformers\n",
    "import keras\n",
    "import shutil\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4b4311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapNewLineReplace(origLine):\n",
    "    newLine = origLine.replace(\"\\n\",\". \")\n",
    "    return(newLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e6e54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupModelInputs(tweetData,debug=False):\n",
    "    tweetText = list(tweetData['text'])\n",
    "    tweetText = list(map(mapNewLineReplace,tweetText))\n",
    "    inputs = TOKEN(tweetText,max_length=100,truncation=True,padding='max_length',return_tensors=\"tf\")\n",
    "    inp_ids = tf.convert_to_tensor(inputs['input_ids'])\n",
    "    inputs['input_ids'] = inp_ids\n",
    "    return(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bcbac5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/mnt/h/Aspire/BERT/testSaveBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30623\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modelFile = '/mnt/h/Aspire/BERT/child/checkpiont_3layers/age_model_checkpoint.h5'\n",
    "modelFile = '/mnt/h/Aspire/BERT/health/health_model_checkpoint.h5'\n",
    "final_BERT = tf.keras.models.load_model(modelFile,custom_objects={\"TFBertModel\": transformers.TFBertModel})\n",
    "final_BERT.compile(loss=[tf.keras.losses.BinaryCrossentropy()],optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0965c039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Adding ðŸ˜‚ to the vocabulary\n",
      "Adding â¤ to the vocabulary\n",
      "Adding ðŸ˜­ to the vocabulary\n",
      "Adding ðŸ˜ to the vocabulary\n",
      "Adding ðŸŽ‰ to the vocabulary\n",
      "Adding ðŸ˜Š to the vocabulary\n",
      "Adding ðŸ˜˜ to the vocabulary\n",
      "Adding ðŸ˜© to the vocabulary\n",
      "Adding ðŸ’• to the vocabulary\n",
      "Adding ðŸ½ to the vocabulary\n",
      "Adding ðŸ¼ to the vocabulary\n",
      "Adding ðŸ¾ to the vocabulary\n",
      "Adding ðŸ’¯ to the vocabulary\n",
      "Adding ðŸ”¥ to the vocabulary\n",
      "Adding ðŸ» to the vocabulary\n",
      "Adding ðŸ™ to the vocabulary\n",
      "Adding ðŸ™Œ to the vocabulary\n",
      "Adding ðŸ¤£ to the vocabulary\n",
      "Adding ðŸ˜’ to the vocabulary\n",
      "Adding ðŸ’™ to the vocabulary\n",
      "Adding â™€ to the vocabulary\n",
      "Adding ðŸ™„ to the vocabulary\n",
      "Adding ðŸ˜ to the vocabulary\n",
      "Adding ðŸ‘ to the vocabulary\n",
      "Adding â˜º to the vocabulary\n",
      "Adding ðŸ’€ to the vocabulary\n",
      "Adding ðŸ‘Œ to the vocabulary\n",
      "Adding ðŸŽˆ to the vocabulary\n",
      "Adding ðŸ’œ to the vocabulary\n",
      "Adding ðŸ’— to the vocabulary\n",
      "Adding ðŸ˜… to the vocabulary\n",
      "Adding ðŸ’– to the vocabulary\n",
      "Adding ðŸ¤” to the vocabulary\n",
      "Adding ðŸ˜Ž to the vocabulary\n",
      "Adding â™‚ to the vocabulary\n",
      "Adding ðŸ˜” to the vocabulary\n",
      "Adding ðŸ¤¦ to the vocabulary\n",
      "Adding ðŸ™ƒ to the vocabulary\n",
      "Adding ðŸ‘ to the vocabulary\n",
      "Adding ðŸ¤· to the vocabulary\n",
      "Adding âœ¨ to the vocabulary\n",
      "Adding ðŸ˜ to the vocabulary\n",
      "Adding ðŸŽ¶ to the vocabulary\n",
      "Adding ðŸŽ‚ to the vocabulary\n",
      "Adding ðŸ’ª to the vocabulary\n",
      "Adding ðŸ’› to the vocabulary\n",
      "Adding ðŸ˜Œ to the vocabulary\n",
      "Adding ðŸŽŠ to the vocabulary\n",
      "Adding ðŸ˜³ to the vocabulary\n",
      "Adding ðŸ‘€ to the vocabulary\n",
      "Adding ðŸ˜‰ to the vocabulary\n",
      "Adding ðŸ˜¢ to the vocabulary\n",
      "Adding â€¼ to the vocabulary\n",
      "Adding ðŸ˜‹ to the vocabulary\n",
      "Adding ðŸ’” to the vocabulary\n",
      "Adding ðŸ’˜ to the vocabulary\n",
      "Adding âœŒ to the vocabulary\n",
      "Adding ðŸ˜ˆ to the vocabulary\n",
      "Adding ðŸ˜´ to the vocabulary\n",
      "Adding ðŸ¥° to the vocabulary\n",
      "Adding ðŸ’‹ to the vocabulary\n",
      "Adding ðŸ’ž to the vocabulary\n",
      "Adding ðŸ¤— to the vocabulary\n",
      "Adding ðŸ¥º to the vocabulary\n",
      "Adding ðŸ’š to the vocabulary\n",
      "Adding ðŸŽ to the vocabulary\n",
      "Adding âœŠ to the vocabulary\n",
      "Adding ðŸ˜‘ to the vocabulary\n",
      "Adding ðŸ˜« to the vocabulary\n",
      "Adding ðŸ˜œ to the vocabulary\n",
      "Adding ðŸ’ to the vocabulary\n",
      "Adding ðŸ’“ to the vocabulary\n",
      "Adding ðŸ˜¡ to the vocabulary\n",
      "Adding ðŸ˜ to the vocabulary\n",
      "Adding ðŸ¤˜ to the vocabulary\n",
      "Adding ðŸ˜• to the vocabulary\n",
      "Adding ðŸ˜¤ to the vocabulary\n",
      "Adding â˜€ to the vocabulary\n",
      "Adding ðŸ˜„ to the vocabulary\n",
      "Adding ðŸ˜› to the vocabulary\n",
      "Adding ðŸ™ˆ to the vocabulary\n",
      "Adding ðŸ˜ž to the vocabulary\n",
      "Adding ðŸ˜» to the vocabulary\n",
      "Adding ðŸ» to the vocabulary\n",
      "Adding ðŸ€ to the vocabulary\n",
      "Adding ðŸ˜¬ to the vocabulary\n",
      "Adding ðŸ˜‡ to the vocabulary\n",
      "Adding ðŸ–¤ to the vocabulary\n",
      "Adding ðŸ˜ª to the vocabulary\n",
      "Adding ðŸˆ to the vocabulary\n",
      "Adding ðŸ™‚ to the vocabulary\n",
      "Adding ðŸ‘‘ to the vocabulary\n",
      "Adding ðŸ’ƒ to the vocabulary\n",
      "Adding ðŸ˜† to the vocabulary\n",
      "Adding ðŸš¨ to the vocabulary\n",
      "Adding ðŸ‘Š to the vocabulary\n",
      "Adding ðŸ˜€ to the vocabulary\n",
      "Adding ðŸ˜ƒ to the vocabulary\n",
      "Adding ðŸ˜ to the vocabulary\n",
      "Adding ðŸ¥³ to the vocabulary\n",
      "Adding ðŸ˜· to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/25 [==============================] - 2s 38ms/step\n",
      "[[0 0 0 0 0]\n",
      " [0 0 1 1 0]\n",
      " [0 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "TOKEN = BertTokenizer.from_pretrained('expandedTokenBase')\n",
    "b = setupModelInputs(imageTest)\n",
    "preds = final_BERT.predict([b['input_ids'],b['attention_mask']])\n",
    "predsInt = (preds>0)*1\n",
    "print(predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "838bfc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 0 0 1 2 1 1 1 2 1 2 1 2 2 0 2 1 2 1 1 1 2 1 1 0 2 0 1 1 2 1 1 2 1 2\n",
      " 0 2 1 0 2 1 0 1 1 2 0 1 1 1 2 0 1 0 2 0 1 0 2 2 0 1 2 1 2 1 0 0 1 1 2 2 2\n",
      " 0 2 0 2 2 0 1 2 2 2 2 0 2 1 2 0 0 1 2 1 2 1 0 0 2 0 2 1 1 2 2 0 1 2 2 1 1\n",
      " 1 1 1 1 2 2 2 1 1 1 1 2 1 1 1 2 2 1 2 2 1 1 2 1 1 2 2 2 1 2 2 0 1 1 1 2 2\n",
      " 1 0 1 0 2 0 1 1 1 1 2 2 1 2 2 0 2 1 1 1 0 2 2 1 1 1 2 0 0 1 0 1 1 0 1 1 1\n",
      " 2 0 2 0 0 1 1 1 1 0 1 2 1 2 1 2 1 0 0 1 0 0 2 0 1 0 0 1 2 2 1 1 1 0 2 0 1\n",
      " 2 2 2 2 0 2 0 1 2 1 1 2 2 0 2 1 1 2 1 0 2 1 1 1 2 2 0 2 1 1 0 1 2 0 1 2 2\n",
      " 2 2 2 1 1 2 2 0 2 0 2 2 2 1 1 0 2 1 2 1 1 0 0 0 0 1 2 1 2 1 1 2 1 1 2 0 1\n",
      " 1 0 2 1 1 1 0 2 1 1 1 0 2 2 0 2 1 1 0 1 1 2 1 1 2 2 1 2 2 2 2 1 0 1 1 3 1\n",
      " 2 1 1 1 2 2 2 1 2 0 1 1 1 2 0 0 2 1 2 1 0 1 0 2 0 2 0 1 2 0 2 0 2 1 0 2 1\n",
      " 2 1 1 1 2 0 1 2 2 1 2 1 1 0 0 1 1 1 2 1 2 0 0 0 2 2 0 1 2 2 1 0 1 0 0 1 2\n",
      " 0 0 0 1 0 0 2 1 2 3 0 2 1 2 2 2 0 2 0 0 0 1 0 2 1 2 1 2 0 0 1 0 2 1 2 0 2\n",
      " 2 2 2 1 1 2 0 0 2 2 0 2 1 0 2 0 1 1 2 2 2 1 2 0 1 2 1 1 2 0 1 1 1 1 2 2 0\n",
      " 2 2 1 2 1 2 0 1 0 2 2 0 2 2 1 1 0 1 2 0 0 2 0 2 1 1 2 0 1 0 2 2 0 1 2 0 0\n",
      " 1 0 1 2 2 0 0 0 2 2 1 1 0 2 1 2 0 2 1 0 0 1 0 1 1 1 1 0 2 1 1 2 1 2 2 1 2\n",
      " 1 0 0 2 2 0 0 2 2 1 0 1 1 2 2 1 0 1 2 2 1 0 2 0 1 0 0 2 0 2 2 0 2 0 2 1 1\n",
      " 0 1 1 1 1 2 0 2 2 1 0 1 2 2 1 2 2 1 0 2 2 2 0 0 2 2 2 1 1 2 1 2 1 0 2 1 0\n",
      " 1 1 2 1 2 2 1 0 1 1 2 1 2 0 1 2 1 2 1 1 1 0 1 2 1 1 1 1 2 0 2 2 1 0 2 0 1\n",
      " 2 2 0 0 0 2 0 1 0 1 1 2 2 1 2 1 1 0 2 1 2 2 1 1 0 2 2 2 2 0 1 1 1 0 2 0 1\n",
      " 0 1 1 2 2 2 1 2 0 0 2 1 0 0 2 0 0 2 2 0 1 2 0 1 2 0 2 0 0 2 0 1 2 0 1 1 2\n",
      " 2 1 0 1 0 1 1 0 2 2 0 0 2 0 1 2 2 2 1 1 2 2 2 2 0 1 1 1 1 1 1 0 1 0 1 2 1\n",
      " 1 2 1 1 1 1 1 1 0 0 2 2 1 2 1 2 1 1 2 0 2 2 2 1 1 1 2 0 0 2 2 1 1 2 1 1 2\n",
      " 1 2 1 2 2 0 2 0 1 1 0 0 2 0 2 1 2 1 0 2 0 2 0 0 2 1 0 1 1 1 1 0 0 2 2 1 0\n",
      " 2 1 1 1 2 2 1 0 1 0 0 2 2 2 1 0 1 2 2 2 0 1 0 1 1 0 0 0 3 0 1 1 0 2 1 2 0\n",
      " 0 1 2 0 1 0 2 1 1 0 2 2 1 2 0 1 2 1 1 2 1 1 0 1 2 1 1 2 1 0 2 1 1 1 3 2 0\n",
      " 2 1 1 1 2 1 0 0 0 2 2 1 0 1 1 1 1 2 2 2 0 2 0 2 2 2 1 1 0 2 1 1 2 2 1 2 0\n",
      " 1 1 0 0 2 2 2 0 2 1 1 2 1 2 0 0 1 0 0 1 1 2 0 0 2 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(predsInt,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2b4c4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthCodingDict = {\n",
    "            'Cognitive health':0,\n",
    "            'Emotional/social health':1,\n",
    "            'Physical health':2,\n",
    "            'Positive impact':0,\n",
    "            'Negative impact':1,\n",
    "        }\n",
    "\n",
    "def convertPlaceCodeToLabels(placeCode):\n",
    "    keys = list(healthCodingDict.keys())\n",
    "    codeArr = [0 for x in range(3)]\n",
    "    for key in keys:\n",
    "        if key in placeCode:\n",
    "            codeArr[healthCodingDict[key]] = 1\n",
    "    return(np.asarray(codeArr))\n",
    "    \n",
    "def convertOutdoorCodeToLabels(outdoorCode):\n",
    "    keys = list(healthCodingDict.keys())\n",
    "    codeArr = [0 for x in range(2)]\n",
    "    for key in keys:\n",
    "        if key in outdoorCode:\n",
    "            codeArr[healthCodingDict[key]] = 1\n",
    "    return(np.asarray(codeArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "964d823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  4,  72, 100,  87,  88]),\n",
       " array([14.28571429, 35.82089552, 74.07407407, 42.64705882, 68.21705426]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcTruePositive(labels,predictions):\n",
    "    correct = np.multiply(labels,predictions)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTruePositive(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "df99acdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([749, 538, 617, 533, 610]),\n",
       " array([99.4687915 , 92.75862069, 95.51083591, 92.37435009, 93.55828221]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcTrueNegative(labels,predictions):\n",
    "    labZeros = np.where(labels==0,1,0)\n",
    "    predZeros = np.where(predictions==0,1,0)\n",
    "    correct = np.multiply(labZeros,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labZeros,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTrueNegative(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6639cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4 42 29 44 42]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([ 4, 42, 29, 44, 42]),\n",
       " array([14.28571429, 20.89552239, 21.48148148, 21.56862745, 32.55813953]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalsePositive(labels,predictions):\n",
    "    labZeros = np.where(labels==0,1,0)\n",
    "    correct = np.multiply(labZeros,predictions)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    print(nCorrect)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalsePositive(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9229a576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 24, 129,  35, 117,  41]),\n",
       " array([85.71428571, 64.17910448, 25.92592593, 57.35294118, 31.78294574]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalseNegative(labels,predictions):\n",
    "    predZeros = np.where(predictions==0,1,0)\n",
    "    correct = np.multiply(labels,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalseNegative(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc047728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
