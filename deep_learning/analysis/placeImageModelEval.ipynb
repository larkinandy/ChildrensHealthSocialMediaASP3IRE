{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "80a6e14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-02 10:56:25.215468: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-10-02 10:56:25.405703: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-10-02 10:56:26.038107: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64::/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-10-02 10:56:26.039752: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64::/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib:/home/larkinan/miniconda3/envs/tf/lib/:/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/nvidia/cudnn/lib\n",
      "2023-10-02 10:56:26.039759: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "# Setting random seeds to reduce the amount of randomness in the neural net weights and results\n",
    "# The results may still not be exactly reproducible\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8fcf526f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 16:28:07.337614: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.337697: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.337718: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.337945: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.337956: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-29 16:28:07.337988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.338002: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 21290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Testing to ensure GPU is being utilized\n",
    "# Ensure that the Runtime Type for this notebook is set to GPU\n",
    "# If a GPU device is not found, change the runtime type under:\n",
    "# Runtime>> Change runtime type>> Hardware accelerator>> GPU\n",
    "# and run the notebook from the beginning again.\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f45b9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def convertCodeToNumpy(code):\n",
    "    encoding = []\n",
    "    for charIndex in range(len(code)):\n",
    "        curCar = code[charIndex]\n",
    "        if curCar =='0':\n",
    "            encoding.append(0)\n",
    "        elif curCar =='1':\n",
    "            encoding.append(1)\n",
    "    return(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "966020ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            filename         encoding   \n",
      "0  /mnt/h/Aspire/Bert/place/train/images/aug/3_88...  [0 0 0 0 0 0 1]  \\\n",
      "1  /mnt/h/Aspire/Bert/place/train/images/aug/3_88...  [0 0 0 0 0 0 1]   \n",
      "2  /mnt/h/Aspire/Bert/place/train/images/aug/3_88...  [0 0 0 0 0 0 1]   \n",
      "3  /mnt/h/Aspire/Bert/place/train/images/aug/3_88...  [0 0 0 0 0 0 1]   \n",
      "4  /mnt/h/Aspire/Bert/place/train/images/aug/3_88...  [0 0 0 0 0 0 1]   \n",
      "\n",
      "                  target  \n",
      "0  [0, 0, 0, 0, 0, 0, 1]  \n",
      "1  [0, 0, 0, 0, 0, 0, 1]  \n",
      "2  [0, 0, 0, 0, 0, 0, 1]  \n",
      "3  [0, 0, 0, 0, 0, 0, 1]  \n",
      "4  [0, 0, 0, 0, 0, 0, 1]  \n"
     ]
    }
   ],
   "source": [
    "# Importing the augmented training dataset and testing dataset to create tensors of images using the filename paths.\n",
    "train_aug_df = pd.read_csv(\"/mnt/h/Aspire/BERT/place/train/trainData.csv\")\n",
    "test_df = pd.read_csv(\"/mnt/h/Aspire/BERT/place/test/testData.csv\")\n",
    "train_aug_df['filename'] = train_aug_df['filename'].str.replace(\"H:/\", \"/mnt/h/\", case = False)\n",
    "train_aug_df['target'] = train_aug_df['encoding'].map(convertCodeToNumpy)\n",
    "test_df['target'] = test_df['encoding'].map(convertCodeToNumpy)\n",
    "test_df['filename'] = test_df['filename'].str.replace(\"H:/\",\"/mnt/h/\",case=False)\n",
    "test_labels_list = list(test_df['target'])\n",
    "print(train_aug_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "619329d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to read the image, decode the image from given tensor and one-hot encode the image label class.\n",
    "# Changing the channels para in tf.io.decode_jpeg from 3 to 1 changes the output images from RGB coloured to grayscale.\n",
    "num_classes = 7\n",
    "def _parse_function(filename, label):   \n",
    "    image_string = tf.io.read_file(filename)\n",
    "    image_decoded = tf.io.decode_jpeg(image_string, channels=3)    # channels=1 to convert to grayscale, channels=3 to convert to RGB.\n",
    "    #image_decoded = tf.image.resize(image_decoded, [224, 224])\n",
    "    image_decoded = tf.cast(image_decoded, dtype=tf.float32)\n",
    "    \n",
    "    return(image_decoded,label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf2e5455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 16:28:07.781514: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.781603: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.781626: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.781863: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.781887: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.781904: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.782087: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.782094: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1700] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-09-29 16:28:07.782122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-09-29 16:28:07.782134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 21290 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4090, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    }
   ],
   "source": [
    "# Converting the filenames and target class labels into lists for augmented train and test datasets.\n",
    "train_aug_filenames_list = list(train_aug_df['filename'])\n",
    "train_aug_labels_list = list(train_aug_df['target'])\n",
    "test_filenames_list = list(test_df['filename'])\n",
    "test_labels_list = list(test_df['target'])\n",
    "# Creating tensorflow constants of filenames and labels for augmented train and test datasets from the lists defined above.\n",
    "train_aug_filenames_tensor = tf.constant(train_aug_filenames_list)\n",
    "train_aug_labels_tensor = tf.constant(train_aug_labels_list)\n",
    "test_filenames_tensor = tf.constant(test_filenames_list)\n",
    "test_labels_tensor = tf.constant(test_labels_list)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_filenames_tensor, test_labels_tensor))\n",
    "test_dataset = test_dataset.map(_parse_function)\n",
    "# test_dataset = test_dataset.repeat(3)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)    # Same as batch_size hyperparameter in model.fit() below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3371047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "2023-09-29 16:28:10.478642: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8500\n",
      "2023-09-29 16:28:10.991758: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_s16_fe/1\", trainable=True),\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b16_fe/1\", trainable=True),\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_b32_fe/1\", trainable=False),\n",
    "     #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_r26_s32_lightaug_fe/1\", trainable=True),\n",
    "    #hub.KerasLayer(\"https://tfhub.dev/sayakpaul/vit_r50_l32_fe/1\", trainable=True),\n",
    "hub.KerasLayer(\"https://tfhub.dev/google/imagenet/resnet_v2_101/feature_vector/5\",\n",
    "               trainable=False, arguments=dict(batch_norm_momentum=0.997)),\n",
    "    tf.keras.layers.Dense(100,activation='relu'),\n",
    "    tf.keras.layers.Dense(7,activation='sigmoid')\n",
    "])\n",
    "model.compile(\n",
    "    loss=[tf.keras.losses.BinaryCrossentropy(from_logits=True)],\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),metrics=['accuracy'])#,run_eagerly=True)\n",
    "weightFile = \"/mnt/h/Aspire/BERT/place/place_image_model_checkpoint.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath=weightFile,\n",
    "                                monitor='val_loss',\n",
    "                                save_best_only=True,\n",
    "                                save_weights_only=True,\n",
    "                                verbose=1,\n",
    "                                initial_value_threshold=  None#0.97828\n",
    "                                )\n",
    "model(np.zeros((1,224,224,3)))\n",
    "if(os.path.exists(weightFile)):\n",
    "    model.load_weights(weightFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35dc89b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 69ms/step\n",
      "[0.02541909 0.11607458 0.26550087 0.11254267 0.06386312 0.2380479\n",
      " 0.75822085]\n",
      "[0.41196102 0.12263011 0.35446662 0.1695063  0.07724446 0.8379662\n",
      " 0.77491796]\n"
     ]
    }
   ],
   "source": [
    "# Generating predictions from the model above.\n",
    "placeImagePred = model.predict(test_dataset)\n",
    "#final_cnn_pred = final_cnn.predict(train_aug_dataset)\n",
    "print(placeImagePred[0])\n",
    "imgBinary = (placeImagePred>0.5)*1\n",
    "print(np.max(placeImagePred,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3dab8f1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgBinary[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f4aaa6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'predict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m             predictions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((predictions,pred))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m([labelSet,predictions])\n\u001b[0;32m---> 13\u001b[0m labels,predictions \u001b[38;5;241m=\u001b[39m \u001b[43mgetPredictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43mplaceImagePred\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m, in \u001b[0;36mgetPredictions\u001b[0;34m(dataset, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m labelSet,predictions \u001b[38;5;241m=\u001b[39m [],[]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text, labels \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m(text)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(\u001b[38;5;28mlen\u001b[39m(labelSet)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m      7\u001b[0m         labelSet \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'predict'"
     ]
    }
   ],
   "source": [
    "#BERT_pred = final_BERT.predict(testData)\n",
    "def getPredictions(dataset,model):\n",
    "    labelSet,predictions = [],[]\n",
    "    for text, labels in dataset.take(1000):\n",
    "        pred = model.predict(text)\n",
    "        if(len(labelSet)==0):\n",
    "            labelSet = labels.numpy()\n",
    "            predictions = pred\n",
    "        else:\n",
    "            labelSet = np.concatenate((labelSet,labels))\n",
    "            predictions = np.concatenate((predictions,pred))\n",
    "    return([labelSet,predictions])\n",
    "labels,predictions = getPredictions(test_dataset,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b01c2385",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 35,  53, 225, 141,  21, 465, 321])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(test_labels_list,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfbd722c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0., 155., 230.]),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        33.33333333, 71.65109034]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcTruePositive(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    correct = np.multiply(labels,predictionsRound)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTruePositive(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "95eb875e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([955, 937, 765, 849, 969, 476, 515]),\n",
       " array([100.        , 100.        , 100.        , 100.        ,\n",
       "        100.        ,  90.66666667,  76.98056801]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_list = np.asarray(test_labels_list)\n",
    "def calcTrueNegative(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    labZeros = np.where(labels<1,1,0)\n",
    "    predZeros = np.where(predictionsRound==0,1,0)\n",
    "    correct = np.multiply(labZeros,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labZeros,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTrueNegative(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0662a9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 35,  53, 225, 141,  21, 310,  91]),\n",
       " array([100.        , 100.        , 100.        , 100.        ,\n",
       "        100.        ,  66.66666667,  28.34890966]))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalseNegative(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    predZeros = np.where(predictionsRound==0,1,0)\n",
    "    correct = np.multiply(labels,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalseNegative(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e7f413d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0.   0.   0.   0.   0.  49. 154.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  0.,   0.,   0.,   0.,   0.,  49., 154.]),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        10.53763441, 47.97507788]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalsePositive(labels,predictions):\n",
    "    predictionsRound = tf.round(predictions)\n",
    "    labZeros = np.where(labels==0,1,0)\n",
    "    correct = np.multiply(labZeros,predictionsRound)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    print(nCorrect)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalsePositive(test_labels_list,placeImagePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a79f0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79/4016371268.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  testData['isImage'] = isImage\n"
     ]
    }
   ],
   "source": [
    "import pandas as ps\n",
    "import os\n",
    "allData = ps.read_csv(\"/mnt/h/Aspire/BERT/place/placeLabelsTrainTest_Sep26_23.csv\")\n",
    "testData = allData[allData['test']==1]\n",
    "isImage = []\n",
    "for curNum in range(testData.count()[0]):\n",
    "    curRecord = testData.iloc[curNum]\n",
    "    curImage = curRecord['img_http']\n",
    "    if(os.path.exists(\"/mnt/h/Aspire/BERT/place/test/images/base/\" + curImage[:-4] + \"_padded.jpg\")):\n",
    "        isImage.append(1)\n",
    "    else:\n",
    "        isImage.append(0)\n",
    "testData['isImage'] = isImage\n",
    "imageTest = testData[testData['isImage']==1]\n",
    "imageTest.to_csv('/mnt/h/Aspire/BERT/place/test/imageTweets.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "fc5ea57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       Lots of signs outside the Nannie Lee Center in...\n",
      "20      First game as a high schooler today!! #sisterl...\n",
      "23      Caregivers, please complete the reopening surv...\n",
      "30      Yes a thousand times yes!!! Especially for us ...\n",
      "40      @DCCCAInc CEO @LoriAlvarado1 says their women'...\n",
      "                              ...                        \n",
      "4955    “@Lil_Hands_Sign: Signing children tend to be ...\n",
      "4969    #1stgrade Ss teaching Kindergarten Ss how to u...\n",
      "4982    “@BarackObama: Tell a friend, tell a family me...\n",
      "4985    OMG #avis rental car line. This is a nightmare...\n",
      "4990    Congrats @callanmccarthy on your last ever hig...\n",
      "Name: text, Length: 990, dtype: object\n",
      "[[0 0 0 0 0 0 1]\n",
      " [0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0]\n",
      " [1 0 0 1 0 0 1]\n",
      " [0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "a = imageTest['text']\n",
    "print(a)\n",
    "tweetText = list(imageTest['text'])\n",
    "tweetText = list(map(mapNewLineReplace,tweetText))\n",
    "tweetLabels = list(map(convertPlaceCodeToLabels,list(imageTest['location'])))\n",
    "tweetLabels2 = list(map(convertOutdoorCodeToLabels,list(imageTest['location_cat'])))\n",
    "tweetLabels3 = np.concatenate((tweetLabels, tweetLabels2),axis=1)\n",
    "print(tweetLabels3[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0aa14f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
      "  warnings.warn(\n",
      "/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "/home/larkinan/miniconda3/envs/tf/lib/python3.9/site-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import TFGenerationMixin` instead.\n",
      "  warnings.warn(\n",
      "Loading custom CUDA kernels...\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Loading custom CUDA kernels...\n",
      "Using /home/larkinan/.cache/torch_extensions/py39_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/larkinan/.cache/torch_extensions/py39_cu117/cuda_kernel/build.ninja...\n",
      "Building extension module cuda_kernel...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module cuda_kernel...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Conv2D, AveragePooling2D, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras import utils \n",
    "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
    "import glob\n",
    "\n",
    "# Setting random seeds to reduce the amount of randomness in the neural net weights and results\n",
    "# The results may still not be exactly reproducible\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from transformers import *\n",
    "import transformers\n",
    "import keras\n",
    "import shutil\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b4b4311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapNewLineReplace(origLine):\n",
    "    newLine = origLine.replace(\"\\n\",\". \")\n",
    "    return(newLine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8e6e54be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupModelInputs(tweetData,debug=False):\n",
    "    tweetText = list(tweetData['text'])\n",
    "    tweetText = list(map(mapNewLineReplace,tweetText))\n",
    "    inputs = TOKEN(tweetText,max_length=100,truncation=True,padding='max_length',return_tensors=\"tf\")\n",
    "    inp_ids = tf.convert_to_tensor(inputs['input_ids'])\n",
    "    inputs['input_ids'] = inp_ids\n",
    "    return(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bcbac5ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"/mnt/h/Aspire/BERT/testSaveBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.33.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 30623\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#modelFile = '/mnt/h/Aspire/BERT/child/checkpiont_3layers/age_model_checkpoint.h5'\n",
    "modelFile = '/mnt/h/Aspire/BERT/place/place_model_checkpoint.h5'\n",
    "final_BERT = tf.keras.models.load_model(modelFile,custom_objects={\"TFBertModel\": transformers.TFBertModel})\n",
    "final_BERT.compile(loss=[tf.keras.losses.BinaryCrossentropy()],optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0965c039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.txt\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "Adding 😂 to the vocabulary\n",
      "Adding ❤ to the vocabulary\n",
      "Adding 😭 to the vocabulary\n",
      "Adding 😍 to the vocabulary\n",
      "Adding 🎉 to the vocabulary\n",
      "Adding 😊 to the vocabulary\n",
      "Adding 😘 to the vocabulary\n",
      "Adding 😩 to the vocabulary\n",
      "Adding 💕 to the vocabulary\n",
      "Adding 🏽 to the vocabulary\n",
      "Adding 🏼 to the vocabulary\n",
      "Adding 🏾 to the vocabulary\n",
      "Adding 💯 to the vocabulary\n",
      "Adding 🔥 to the vocabulary\n",
      "Adding 🏻 to the vocabulary\n",
      "Adding 🙏 to the vocabulary\n",
      "Adding 🙌 to the vocabulary\n",
      "Adding 🤣 to the vocabulary\n",
      "Adding 😒 to the vocabulary\n",
      "Adding 💙 to the vocabulary\n",
      "Adding ♀ to the vocabulary\n",
      "Adding 🙄 to the vocabulary\n",
      "Adding 😁 to the vocabulary\n",
      "Adding 👏 to the vocabulary\n",
      "Adding ☺ to the vocabulary\n",
      "Adding 💀 to the vocabulary\n",
      "Adding 👌 to the vocabulary\n",
      "Adding 🎈 to the vocabulary\n",
      "Adding 💜 to the vocabulary\n",
      "Adding 💗 to the vocabulary\n",
      "Adding 😅 to the vocabulary\n",
      "Adding 💖 to the vocabulary\n",
      "Adding 🤔 to the vocabulary\n",
      "Adding 😎 to the vocabulary\n",
      "Adding ♂ to the vocabulary\n",
      "Adding 😔 to the vocabulary\n",
      "Adding 🤦 to the vocabulary\n",
      "Adding 🙃 to the vocabulary\n",
      "Adding 👍 to the vocabulary\n",
      "Adding 🤷 to the vocabulary\n",
      "Adding ✨ to the vocabulary\n",
      "Adding 😏 to the vocabulary\n",
      "Adding 🎶 to the vocabulary\n",
      "Adding 🎂 to the vocabulary\n",
      "Adding 💪 to the vocabulary\n",
      "Adding 💛 to the vocabulary\n",
      "Adding 😌 to the vocabulary\n",
      "Adding 🎊 to the vocabulary\n",
      "Adding 😳 to the vocabulary\n",
      "Adding 👀 to the vocabulary\n",
      "Adding 😉 to the vocabulary\n",
      "Adding 😢 to the vocabulary\n",
      "Adding ‼ to the vocabulary\n",
      "Adding 😋 to the vocabulary\n",
      "Adding 💔 to the vocabulary\n",
      "Adding 💘 to the vocabulary\n",
      "Adding ✌ to the vocabulary\n",
      "Adding 😈 to the vocabulary\n",
      "Adding 😴 to the vocabulary\n",
      "Adding 🥰 to the vocabulary\n",
      "Adding 💋 to the vocabulary\n",
      "Adding 💞 to the vocabulary\n",
      "Adding 🤗 to the vocabulary\n",
      "Adding 🥺 to the vocabulary\n",
      "Adding 💚 to the vocabulary\n",
      "Adding 🎁 to the vocabulary\n",
      "Adding ✊ to the vocabulary\n",
      "Adding 😑 to the vocabulary\n",
      "Adding 😫 to the vocabulary\n",
      "Adding 😜 to the vocabulary\n",
      "Adding 💁 to the vocabulary\n",
      "Adding 💓 to the vocabulary\n",
      "Adding 😡 to the vocabulary\n",
      "Adding 😐 to the vocabulary\n",
      "Adding 🤘 to the vocabulary\n",
      "Adding 😕 to the vocabulary\n",
      "Adding 😤 to the vocabulary\n",
      "Adding ☀ to the vocabulary\n",
      "Adding 😄 to the vocabulary\n",
      "Adding 😛 to the vocabulary\n",
      "Adding 🙈 to the vocabulary\n",
      "Adding 😞 to the vocabulary\n",
      "Adding 😻 to the vocabulary\n",
      "Adding 🍻 to the vocabulary\n",
      "Adding 🏀 to the vocabulary\n",
      "Adding 😬 to the vocabulary\n",
      "Adding 😇 to the vocabulary\n",
      "Adding 🖤 to the vocabulary\n",
      "Adding 😪 to the vocabulary\n",
      "Adding 🏈 to the vocabulary\n",
      "Adding 🙂 to the vocabulary\n",
      "Adding 👑 to the vocabulary\n",
      "Adding 💃 to the vocabulary\n",
      "Adding 😆 to the vocabulary\n",
      "Adding 🚨 to the vocabulary\n",
      "Adding 👊 to the vocabulary\n",
      "Adding 😀 to the vocabulary\n",
      "Adding 😃 to the vocabulary\n",
      "Adding 😝 to the vocabulary\n",
      "Adding 🥳 to the vocabulary\n",
      "Adding 😷 to the vocabulary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/31 [==============================] - 1s 39ms/step\n",
      "[[0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "TOKEN = BertTokenizer.from_pretrained('expandedTokenBase')\n",
    "b = setupModelInputs(imageTest)\n",
    "preds = final_BERT.predict([b['input_ids'],b['attention_mask']])\n",
    "predsInt = (preds>0)*1\n",
    "print(predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "838bfc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 0 0 1 2 1 1 1 2 1 2 1 2 2 0 2 1 2 1 1 1 2 1 1 0 2 0 1 1 2 1 1 2 1 2\n",
      " 0 2 1 0 2 1 0 1 1 2 0 1 1 1 2 0 1 0 2 0 1 0 2 2 0 1 2 1 2 1 0 0 1 1 2 2 2\n",
      " 0 2 0 2 2 0 1 2 2 2 2 0 2 1 2 0 0 1 2 1 2 1 0 0 2 0 2 1 1 2 2 0 1 2 2 1 1\n",
      " 1 1 1 1 2 2 2 1 1 1 1 2 1 1 1 2 2 1 2 2 1 1 2 1 1 2 2 2 1 2 2 0 1 1 1 2 2\n",
      " 1 0 1 0 2 0 1 1 1 1 2 2 1 2 2 0 2 1 1 1 0 2 2 1 1 1 2 0 0 1 0 1 1 0 1 1 1\n",
      " 2 0 2 0 0 1 1 1 1 0 1 2 1 2 1 2 1 0 0 1 0 0 2 0 1 0 0 1 2 2 1 1 1 0 2 0 1\n",
      " 2 2 2 2 0 2 0 1 2 1 1 2 2 0 2 1 1 2 1 0 2 1 1 1 2 2 0 2 1 1 0 1 2 0 1 2 2\n",
      " 2 2 2 1 1 2 2 0 2 0 2 2 2 1 1 0 2 1 2 1 1 0 0 0 0 1 2 1 2 1 1 2 1 1 2 0 1\n",
      " 1 0 2 1 1 1 0 2 1 1 1 0 2 2 0 2 1 1 0 1 1 2 1 1 2 2 1 2 2 2 2 1 0 1 1 3 1\n",
      " 2 1 1 1 2 2 2 1 2 0 1 1 1 2 0 0 2 1 2 1 0 1 0 2 0 2 0 1 2 0 2 0 2 1 0 2 1\n",
      " 2 1 1 1 2 0 1 2 2 1 2 1 1 0 0 1 1 1 2 1 2 0 0 0 2 2 0 1 2 2 1 0 1 0 0 1 2\n",
      " 0 0 0 1 0 0 2 1 2 3 0 2 1 2 2 2 0 2 0 0 0 1 0 2 1 2 1 2 0 0 1 0 2 1 2 0 2\n",
      " 2 2 2 1 1 2 0 0 2 2 0 2 1 0 2 0 1 1 2 2 2 1 2 0 1 2 1 1 2 0 1 1 1 1 2 2 0\n",
      " 2 2 1 2 1 2 0 1 0 2 2 0 2 2 1 1 0 1 2 0 0 2 0 2 1 1 2 0 1 0 2 2 0 1 2 0 0\n",
      " 1 0 1 2 2 0 0 0 2 2 1 1 0 2 1 2 0 2 1 0 0 1 0 1 1 1 1 0 2 1 1 2 1 2 2 1 2\n",
      " 1 0 0 2 2 0 0 2 2 1 0 1 1 2 2 1 0 1 2 2 1 0 2 0 1 0 0 2 0 2 2 0 2 0 2 1 1\n",
      " 0 1 1 1 1 2 0 2 2 1 0 1 2 2 1 2 2 1 0 2 2 2 0 0 2 2 2 1 1 2 1 2 1 0 2 1 0\n",
      " 1 1 2 1 2 2 1 0 1 1 2 1 2 0 1 2 1 2 1 1 1 0 1 2 1 1 1 1 2 0 2 2 1 0 2 0 1\n",
      " 2 2 0 0 0 2 0 1 0 1 1 2 2 1 2 1 1 0 2 1 2 2 1 1 0 2 2 2 2 0 1 1 1 0 2 0 1\n",
      " 0 1 1 2 2 2 1 2 0 0 2 1 0 0 2 0 0 2 2 0 1 2 0 1 2 0 2 0 0 2 0 1 2 0 1 1 2\n",
      " 2 1 0 1 0 1 1 0 2 2 0 0 2 0 1 2 2 2 1 1 2 2 2 2 0 1 1 1 1 1 1 0 1 0 1 2 1\n",
      " 1 2 1 1 1 1 1 1 0 0 2 2 1 2 1 2 1 1 2 0 2 2 2 1 1 1 2 0 0 2 2 1 1 2 1 1 2\n",
      " 1 2 1 2 2 0 2 0 1 1 0 0 2 0 2 1 2 1 0 2 0 2 0 0 2 1 0 1 1 1 1 0 0 2 2 1 0\n",
      " 2 1 1 1 2 2 1 0 1 0 0 2 2 2 1 0 1 2 2 2 0 1 0 1 1 0 0 0 3 0 1 1 0 2 1 2 0\n",
      " 0 1 2 0 1 0 2 1 1 0 2 2 1 2 0 1 2 1 1 2 1 1 0 1 2 1 1 2 1 0 2 1 1 1 3 2 0\n",
      " 2 1 1 1 2 1 0 0 0 2 2 1 0 1 1 1 1 2 2 2 0 2 0 2 2 2 1 1 0 2 1 1 2 2 1 2 0\n",
      " 1 1 0 0 2 2 2 0 2 1 1 2 1 2 0 0 1 0 0 1 1 2 0 0 2 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(np.sum(predsInt,axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2b4c4098",
   "metadata": {},
   "outputs": [],
   "source": [
    "placeCodingDict = {\n",
    "            'Childcare/daycare':0,\n",
    "            'Park/playground/child sports center':1,\n",
    "            'A home':2,\n",
    "            'School':3,\n",
    "            'Neigborhood (but not on home property, etc)':4,\n",
    "            'Indoor location':0,\n",
    "            'Outdoor location':1,\n",
    "        }\n",
    "\n",
    "def convertPlaceCodeToLabels(placeCode):\n",
    "    keys = list(placeCodingDict.keys())\n",
    "    codeArr = [0 for x in range(5)]\n",
    "    for key in keys:\n",
    "        if key in placeCode:\n",
    "            codeArr[placeCodingDict[key]] = 1\n",
    "    return(np.asarray(codeArr))\n",
    "    \n",
    "def convertOutdoorCodeToLabels(outdoorCode):\n",
    "    keys = list(placeCodingDict.keys())\n",
    "    codeArr = [0 for x in range(2)]\n",
    "    for key in keys:\n",
    "        if key in outdoorCode:\n",
    "            codeArr[placeCodingDict[key]] = 1\n",
    "    return(np.asarray(codeArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "964d823c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 13,  30, 127,  92,   9, 317, 202]),\n",
       " array([37.14285714, 56.60377358, 56.44444444, 65.24822695, 42.85714286,\n",
       "        68.17204301, 62.92834891]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcTruePositive(labels,predictions):\n",
    "    correct = np.multiply(labels,predictions)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTruePositive(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "df99acdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([950, 931, 727, 809, 960, 376, 605]),\n",
       " array([99.47643979, 99.35965848, 95.03267974, 95.28857479, 99.07120743,\n",
       "        71.61904762, 90.43348281]))"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcTrueNegative(labels,predictions):\n",
    "    labZeros = np.where(labels==0,1,0)\n",
    "    predZeros = np.where(predictions==0,1,0)\n",
    "    correct = np.multiply(labZeros,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labZeros,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcTrueNegative(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c6639cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  5   6  38  40   9 149  64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([  5,   6,  38,  40,   9, 149,  64]),\n",
       " array([14.28571429, 11.32075472, 16.88888889, 28.36879433, 42.85714286,\n",
       "        32.04301075, 19.9376947 ]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalsePositive(labels,predictions):\n",
    "    labZeros = np.where(labels==0,1,0)\n",
    "    correct = np.multiply(labZeros,predictions)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    print(nCorrect)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalsePositive(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9229a576",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 22,  23,  98,  49,  12, 148, 119]),\n",
       " array([62.85714286, 43.39622642, 43.55555556, 34.75177305, 57.14285714,\n",
       "        31.82795699, 37.07165109]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcFalseNegative(labels,predictions):\n",
    "    predZeros = np.where(predictions==0,1,0)\n",
    "    correct = np.multiply(labels,predZeros)\n",
    "    nCorrect = np.sum(correct,axis=0)\n",
    "    percCorrect = nCorrect/np.sum(labels,axis=0)*100\n",
    "    return(nCorrect,percCorrect)\n",
    "calcFalseNegative(tweetLabels3,predsInt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc047728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
